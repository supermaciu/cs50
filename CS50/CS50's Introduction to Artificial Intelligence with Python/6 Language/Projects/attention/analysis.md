# Analysis

## Layer 4, Head 6

I identified, just as the attention head taking into consideration tokens that
precede them in the Background section, that there is a attention head that looks
for previous token for each one.


Example Sentences:
- "She saw a red [MASK]."
- "And there I saw [MASK] as I quickly ran away"

## Layer 5, Head 8

The attention layer is taking into account adjectives for the masked words, there is
a strong attention layer for word "[MASK]" and "red".

Example Sentences:
- "She saw a red [MASK]."
- "And there I saw [MASK] as I quickly ran away"

